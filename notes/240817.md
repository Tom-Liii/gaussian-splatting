# Analysis on Compact 3DGS
## View-Dependent Color
1. Contract the unbounded positions $p \in R^{N \times 3}$ to the bounded range
2. Compute the 3D view direction $d \in R^3$ for each gaussian based on the camera center point
3. Exploit hash grid followed by a tiny MLP to represent color
    - hash grid:
        1. input position into the hash grids
        2. the resulting feature (color) and the view direction are fed into MLP
    - More formally:
        - $c_n(d) = f(contract(p_n),d;\theta)$
        - $contract(p_n)=$
            - $p_n, \lVert p_n\rVert\leq 1$
            - $(2-\frac{1}{\lVert p_n\rVert})(\frac{p_n}{\lVert p_n\rVert}), \lVert p_n\rVert > 1$
        - where $f(\cdot;\theta)$ stands for the neural field with parameter $\theta$, $contract(\cdot):R^3 \rightarrow R^3$ is the contraction function
4. Use 0-degree component of SH (same number of channel as RGB, but not view-dependent)
5. Convert them into RGB colors due to slightly increase performance compared to representing the RGB color directly

## NeRF
- key idea: represent a 3D scene as a continuous volumetric function parameterized by a neural network
    - *Radiance Field*: The scene is modeled as a continous volumetric
    - *Input*: The network takes as input a *3D coordinates* $(x, y, z)$ and *view direction* $(\theta, \phi)$
    - *Output*: It outputs the *RGB color* and *volume density* at that point
- View-dependent color representation Vs. 3DGS's color representation: 
    - 3DGS: faster rendering, higher fps