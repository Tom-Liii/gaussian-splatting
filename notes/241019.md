# update learning rate
1. the lr is only related to the update of xyz
2. lr is change according to `iteration`
3. should apply in all the 3 stages
# reset opacity
1. maybe can only apply in certain stage, such as stage 1
# the use of `iteration`
1. variables or operations related to iterations:
    - stage 1: 
        - `gaussians.update_learning_rate(iteration)`
        - debug_from setting
        - `tb_writer.add_scalar("every_step_loss", loss.item(), iteration)`
        - progress bar update
        - training report
        - saving iterations
        - `if iteration < opt.densify_until_iter:`
        - `size_threshold = 20 if iteration > opt.opacity_reset_interval else None`
        - `if iteration < opt.iterations:`
        - checkpoints iterations
    - stage 2:
        - all items except `if iteration < opt.densify_until_iter:` in stage 1
    - stage 3:
        - all items except `if iteration < opt.densify_until_iter:` in stage 1